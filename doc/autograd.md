Diferen탵a major캒 fa탵캒 de abordarea ta actual캒 (pe Layere) este c캒 칥n Graf, opera탵iile sunt "atomice". Un strat `Linear` nu mai este o cutie neagr캒, ci este spart 칥n opera탵iile sale matematice de baz캒: 칉nmul탵ire de Matrice (`MatMul`) 탳i Adunare (`Add`).

Iat캒 schema logic캒 a grafului. Cite탳te-o de sus 칥n jos pentru **Forward** 탳i de jos 칥n sus pentru **Backward**.

### Legenda Schemei

* 游릱 **Tensor (Date):** Obiecte care con탵in valori (`data`) 탳i gradien탵i (`grad`).
* 游릭 **Nod Opera탵ie (Function):** Obiecte care 탳tiu matematic캒 (`grad_fn`). Ele leag캒 tensorii 칥ntre ei.
* 拘勇 **Flux Forward:** Crearea tensorilor noi.
* 拘勇 **Flux Backward:** Calea pe care o parcurge `loss.backward()`.

-----

### Graful: Linear -\> ReLU -\> Linear

S캒 presupunem formula: $\hat{y} = W_2 \cdot \text{ReLU}(W_1 \cdot x + b_1) + b_2$

```text
[ FLUXUL DE DATE (Main Stream) ]               [ PARAMETRII (Weights & Biases) ]

           (x) Input                                   (W1)            (b1)
             游릱                                         游릱              游릱
             |                                          |               |
             |                                          |               |
             v                                          v               |
      游릭 [MatMul Node 1] <------------------------------+               |
             |                                                          |
             v                                                          |
      (tmp1) 游릱 (Rezultat x*W1)                                         |
             |                                                          |
             |                                                          |
             v                                                          v
      游릭 [Add Node 1] <-------------------------------------------------+
             |
             v
       (z1)  游릱 (Iesire Layer 1: x*W1 + b1)
             |
             v
      游릭 [ReLU Node]
             |
             v
       (a1)  游릱 (Activari: max(0, z1))                 (W2)            (b2)
             |                                          游릱              游릱
             |                                          |               |
             v                                          v               |
      游릭 [MatMul Node 2] <------------------------------+               |
             |                                                          |
             v                                                          |
      (tmp2) 游릱 (Rezultat a1*W2)                                        |
             |                                                          |
             |                                                          |
             v                                                          v
      游릭 [Add Node 2] <-------------------------------------------------+
             |
             v
      (y_pred) 游릱  (FINAL OUTPUT)
```

```text
[ FLUXUL DE DATE (Main Stream) ]                     [ PARAMETRII (Weights & Biases) ]

              (x) Input
              游릱 游릱 游릱  (Clonat in 3 ramuri)
             /   |   \
            /    |    \
           /     |     \  ----------------------------------------.
          v      v      v                                         |
   (Ramura Q) (Ramura K) (Ramura V)                               |
       |         |          |                                     |
       v         v          v                                     v
 游릭 [MatMul]  游릭 [MatMul] 游릭 [MatMul] <------------------- (W_q, W_k, W_v) 游릱
       |         |          |
       v         v          v                                     v
 游릭 [Add]     游릭 [Add]    游릭 [Add]    <------------------- (b_q, b_k, b_v) 游릱
       |         |          |
    (Q_proj)  (K_proj)   (V_proj)
       游릱        游릱         游릱
       |         |          |
       v         v          v
 游릭 [Split & Transpose] (x3 Nodes)
       |         |          |
     (Q_h)     (K_h)      (V_h)
       游릱        游릱         游릱
       |         |          |
       |         |          |
       +----+----+          |
            |               |
            v               |
     游릭 [Batched MatMul]    |
       (Q_h * K_h^T)        |
            |               |
            v               |
      (Raw Scores) 游릱       |
            |               |
            v               |
     游릭 [Scale & Softmax]   |
            |               |
            v               |
      (Attn Probs) 游릱       |
            |               |
            +-------+-------+
                    |
                    v
            游릭 [Batched MatMul]
           (Attn_Probs * V_h)
                    |
                    v
            (Context Heads) 游릱
                    |
                    v
            游릭 [Transpose & Merge]
                    |
                    v
             (Context Merged)
                    游릱                                          (W_out)
                    |                                             游릱
                    v                                             |
            游릭 [MatMul Final] <-----------------------------------+
                    |
                    v
                 (tmp) 游릱                                       (b_out)
                    |                                             游릱
                    v                                             |
             游릭 [Add Final] <-------------------------------------+
                    |
                    v
                 (Output) 游릱
```

-----

### Cum func탵ioneaz캒 magia (Pointerii)

칉n C++, acest graf nu este desenat undeva 칥ntr-o list캒 global캒. El exist캒 prin **pointeri** stoca탵i 칥n fiecare Tensor creat.

1.  **Forward (Construc탵ia):**

    * C칙nd calculezi `tmp1 = x * W1`, Tensorul rezultat `tmp1` prime탳te un pointer numit `grad_fn` c캒tre un obiect de tip `MatMulBackward`.
    * Acest obiect `MatMulBackward` salveaz캒 pointeri c캒tre p캒rin탵ii lui: `x` 탳i `W1`.
    * C칙nd calculezi `z1 = tmp1 + b1`, Tensorul `z1` prime탳te un pointer c캒tre un obiect `AddBackward`, care 탵ine minte `tmp1` 탳i `b1`.

2.  **Backward (Execu탵ia):**

    * Tu apelezi `y_pred.backward()`.
    * `y_pred` se uit캒 la `grad_fn` al s캒u $\rightarrow$ vede **AddBackward**.
    * **AddBackward** calculeaz캒 gradientul 탳i 칥l trimite la p캒rin탵i:
        * Trimite gradient c캒tre `b2` (care e frunz캒 $\rightarrow$ se opre탳te 탳i acumuleaz캒 칥n `.grad`).
        * Trimite gradient c캒tre `tmp2` (care nu e frunz캒).
    * `tmp2` prime탳te gradientul, se uit캒 la `grad_fn` $\rightarrow$ vede **MatMulBackward**.
    * ... Lan탵ul continu캒 automat p칙n캒 ajunge la `x`, `W1`, `b1`.

### De ce e diferit de ce ai tu?

칉n codul t캒u actual:

* Ai o clas캒 `Linear` care con탵ine 탳i $W$ 탳i $b$ 탳i logica. E o structur캒 "Macro".
* 칉n Graf, $W$ 탳i $b$ sunt doar ni탳te Tensori liberi, iar `Linear` e doar o func탵ie care i-a combinat.

Dac캒 칥n탵elegi aceast캒 diagram캒, 칥n탵elegi esen탵a PyTorch: **Tensorul rezultat 탵ine minte cine l-a f캒cut.**